torch.BFloat16Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.split(tensor: torch.Tensor, split_size_or_sections: Union[int, List[int]], dim: int = 0)
torch.BoolStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.ByteStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.CharStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.ComplexDoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.ComplexFloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.DoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.FloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.Generator(device='cpu')
torch.HalfStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.IntStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.LongStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.QInt32Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.QInt8Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.QUInt4x2Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.QUInt8Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.ShortStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False)
torch.Tensor.abs()
torch.Tensor.abs_()
torch.Tensor.absolute()
torch.Tensor.absolute_()
torch.Tensor.acos()
torch.Tensor.acos_()
torch.Tensor.acosh()
torch.Tensor.acosh_()
torch.Tensor.add(other, *, alpha=1)
torch.Tensor.add_(other, *, alpha=1)
torch.Tensor.addbmm(batch1, batch2, *, beta=1, alpha=1)
torch.Tensor.addbmm_(batch1, batch2, *, beta=1, alpha=1)
torch.Tensor.addcdiv(tensor1, tensor2, *, value=1)
torch.Tensor.addcdiv_(tensor1, tensor2, *, value=1)
torch.Tensor.addcmul(tensor1, tensor2, *, value=1)
torch.Tensor.addcmul_(tensor1, tensor2, *, value=1)
torch.Tensor.addmm(mat1, mat2, *, beta=1, alpha=1)
torch.Tensor.addmm_(mat1, mat2, *, beta=1, alpha=1)
torch.Tensor.addmv(mat, vec, *, beta=1, alpha=1)
torch.Tensor.addmv_(mat, vec, *, beta=1, alpha=1)
torch.Tensor.addr(vec1, vec2, *, beta=1, alpha=1)
torch.Tensor.addr_(vec1, vec2, *, beta=1, alpha=1)
torch.Tensor.all(dim=None, keepdim=False)
torch.Tensor.allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)
torch.Tensor.amax(dim=None, keepdim=False)
torch.Tensor.amin(dim=None, keepdim=False)
torch.Tensor.aminmax(*, dim=None, keepdim=False)
torch.Tensor.angle()
torch.Tensor.any(dim=None, keepdim=False)
torch.Tensor.apply_(callable)
torch.Tensor.arccos()
torch.Tensor.arccos_()
torch.Tensor.arccosh()
torch.Tensor.arccosh_()
torch.Tensor.arcsin()
torch.Tensor.arcsin_()
torch.Tensor.arcsinh()
torch.Tensor.arcsinh_()
torch.Tensor.arctan()
torch.Tensor.arctan_()
torch.Tensor.arctanh()
torch.Tensor.arctanh_(other)
torch.Tensor.argmax(dim=None, keepdim=False)
torch.Tensor.argmin(dim=None, keepdim=False)
torch.Tensor.argsort(dim=-1, descending=False)
torch.Tensor.as_strided(size, stride, storage_offset=None)
torch.Tensor.as_subclass(cls)
torch.Tensor.asin()
torch.Tensor.asin_()
torch.Tensor.asinh()
torch.Tensor.asinh_()
torch.Tensor.atan()
torch.Tensor.atan2(other)
torch.Tensor.atan2_(other)
torch.Tensor.atan_()
torch.Tensor.atanh()
torch.Tensor.atanh_(other)
torch.Tensor.backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None)
torch.Tensor.baddbmm(batch1, batch2, *, beta=1, alpha=1)
torch.Tensor.baddbmm_(batch1, batch2, *, beta=1, alpha=1)
torch.Tensor.bernoulli(*, generator=None)
torch.Tensor.bernoulli_(p=0.5, *, generator=None)
torch.Tensor.bfloat16(memory_format=torch.preserve_format)
torch.Tensor.bincount(weights=None, minlength=0)
torch.Tensor.bitwise_and()
torch.Tensor.bitwise_and_()
torch.Tensor.bitwise_left_shift(other)
torch.Tensor.bitwise_left_shift_(other)
torch.Tensor.bitwise_not()
torch.Tensor.bitwise_not_()
torch.Tensor.bitwise_or()
torch.Tensor.bitwise_or_()
torch.Tensor.bitwise_right_shift(other)
torch.Tensor.bitwise_right_shift_(other)
torch.Tensor.bitwise_xor()
torch.Tensor.bitwise_xor_()
torch.Tensor.bmm(batch2)
torch.Tensor.bool(memory_format=torch.preserve_format)
torch.Tensor.broadcast_to(shape)
torch.Tensor.byte(memory_format=torch.preserve_format)
torch.Tensor.cauchy_(median=0, sigma=1, *, generator=None)
torch.Tensor.ceil()
torch.Tensor.ceil_()
torch.Tensor.char(memory_format=torch.preserve_format)
torch.Tensor.cholesky(upper=False)
torch.Tensor.cholesky_inverse(upper=False)
torch.Tensor.cholesky_solve(input2, upper=False)
torch.Tensor.chunk(chunks, dim=0)
torch.Tensor.clamp(min=None, max=None)
torch.Tensor.clamp_(min=None, max=None)
torch.Tensor.clip(min=None, max=None)
torch.Tensor.clip_(min=None, max=None)
torch.Tensor.clone(*, memory_format=torch.preserve_format)
torch.Tensor.conj()
torch.Tensor.conj_physical()
torch.Tensor.conj_physical_()
torch.Tensor.contiguous(memory_format=torch.contiguous_format)
torch.Tensor.copy_(src, non_blocking=False)
torch.Tensor.copysign(other)
torch.Tensor.copysign_(other)
torch.Tensor.corrcoef()
torch.Tensor.cos()
torch.Tensor.cos_()
torch.Tensor.cosh()
torch.Tensor.cosh_()
torch.Tensor.count_nonzero(dim=None)
torch.Tensor.cov(*, correction=1, fweights=None, aweights=None)
torch.Tensor.cpu(memory_format=torch.preserve_format)
torch.Tensor.cross(other, dim=None)
torch.Tensor.cuda(device=None, non_blocking=False, memory_format=torch.preserve_format)
torch.Tensor.cummax(dim)
torch.Tensor.cummin(dim)
torch.Tensor.cumprod(dim, dtype=None)
torch.Tensor.cumprod_(dim, dtype=None)
torch.Tensor.cumsum(dim, dtype=None)
torch.Tensor.cumsum_(dim, dtype=None)
torch.Tensor.data_ptr()
torch.Tensor.deg2rad()
torch.Tensor.dense_dim()
torch.Tensor.dequantize()
torch.Tensor.det()
torch.Tensor.detach()
torch.Tensor.detach_()
torch.Tensor.device()
torch.Tensor.diag(diagonal=0)
torch.Tensor.diag_embed(offset=0, dim1=-2, dim2=-1)
torch.Tensor.diagflat(offset=0)
torch.Tensor.diagonal(offset=0, dim1=0, dim2=1)
torch.Tensor.diff(n=1, dim=-1, prepend=None, append=None)
torch.Tensor.digamma()
torch.Tensor.digamma_()
torch.Tensor.dim()
torch.Tensor.dist(other, p=2)
torch.Tensor.div(value, *, rounding_mode=None)
torch.Tensor.div_(value, *, rounding_mode=None)
torch.Tensor.divide(value, *, rounding_mode=None)
torch.Tensor.divide_(value, *, rounding_mode=None)
torch.Tensor.dot(other)
torch.Tensor.double(memory_format=torch.preserve_format)
torch.Tensor.dsplit(split_size_or_sections)
torch.Tensor.eig(self, eigenvectors=False)
torch.Tensor.element_size()
torch.Tensor.eq(other)
torch.Tensor.eq_(other)
torch.Tensor.equal(other)
torch.Tensor.erf()
torch.Tensor.erf_()
torch.Tensor.erfc()
torch.Tensor.erfc_()
torch.Tensor.erfinv()
torch.Tensor.erfinv_()
torch.Tensor.exp()
torch.Tensor.exp_()
torch.Tensor.expand(*sizes)
torch.Tensor.expand_as(other)
torch.Tensor.expm1()
torch.Tensor.expm1_()
torch.Tensor.exponential_(lambd=1, *, generator=None)
torch.Tensor.fill_(value)
torch.Tensor.fill_diagonal_(fill_value, wrap=False)
torch.Tensor.fix()
torch.Tensor.fix_()
torch.Tensor.flatten(start_dim=0, end_dim=-1)
torch.Tensor.flip(dims)
torch.Tensor.fliplr()
torch.Tensor.flipud()
torch.Tensor.float(memory_format=torch.preserve_format)
torch.Tensor.float_power(exponent)
torch.Tensor.float_power_(exponent)
torch.Tensor.floor()
torch.Tensor.floor_()
torch.Tensor.floor_divide(value)
torch.Tensor.floor_divide_(value)
torch.Tensor.fmin(other)
torch.Tensor.fmod(divisor)
torch.Tensor.fmod_(divisor)
torch.Tensor.frac()
torch.Tensor.frac_()
torch.Tensor.frexp(input)
torch.Tensor.gather(dim, index)
torch.Tensor.gcd(other)
torch.Tensor.gcd_(other)
torch.Tensor.ge(other)
torch.Tensor.ge_(other)
torch.Tensor.geometric_(p, *, generator=None)
torch.Tensor.geqrf()
torch.Tensor.ger(vec2)
torch.Tensor.get_device()
torch.Tensor.grad()
torch.Tensor.greater(other)
torch.Tensor.greater_(other)
torch.Tensor.greater_equal(other)
torch.Tensor.greater_equal_(other)
torch.Tensor.gt(other)
torch.Tensor.gt_(other)
torch.Tensor.half(memory_format=torch.preserve_format)
torch.Tensor.hardshrink(lambd=0.5)
torch.Tensor.heaviside(values)
torch.Tensor.histc(bins=100, min=0, max=0)
torch.Tensor.histogram(input, bins, *, range=None, weight=None, density=False)
torch.Tensor.hsplit(split_size_or_sections)
torch.Tensor.hypot(other)
torch.Tensor.hypot_(other)
torch.Tensor.i0()
torch.Tensor.i0_()
torch.Tensor.igamma(other)
torch.Tensor.igamma_(other)
torch.Tensor.igammac(other)
torch.Tensor.igammac_(other)
torch.Tensor.imag()
torch.Tensor.index_add(dim, index, source, *, alpha=1)
torch.Tensor.index_add_(dim, index, source, *, alpha=1)
torch.Tensor.index_copy(dim, index, tensor2)
torch.Tensor.index_copy_(dim, index, tensor)
torch.Tensor.index_fill(dim, index, value)
torch.Tensor.index_fill_(dim, index, value)
torch.Tensor.index_put(indices, values, accumulate=False)
torch.Tensor.index_put_(indices, values, accumulate=False)
torch.Tensor.index_select(dim, index)
torch.Tensor.indices()
torch.Tensor.inner(other)
torch.Tensor.int(memory_format=torch.preserve_format)
torch.Tensor.int_repr()
torch.Tensor.inverse()
torch.Tensor.is_complex()
torch.Tensor.is_conj()
torch.Tensor.is_contiguous(memory_format=torch.contiguous_format)
torch.Tensor.is_cuda()
torch.Tensor.is_floating_point()
torch.Tensor.is_inference()
torch.Tensor.is_leaf()
torch.Tensor.is_meta()
torch.Tensor.is_pinned()
torch.Tensor.is_quantized()
torch.Tensor.is_set_to(tensor)
torch.Tensor.is_shared(self)
torch.Tensor.is_signed()
torch.Tensor.is_sparse()
torch.Tensor.isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)
torch.Tensor.isfinite()
torch.Tensor.isinf()
torch.Tensor.isnan()
torch.Tensor.isneginf()
torch.Tensor.isposinf()
torch.Tensor.isreal()
torch.Tensor.istft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False)
torch.Tensor.item()
torch.Tensor.kthvalue(k, dim=None, keepdim=False)
torch.Tensor.lcm(other)
torch.Tensor.lcm_(other)
torch.Tensor.ldexp(other)
torch.Tensor.ldexp_(other)
torch.Tensor.le(other)
torch.Tensor.le_(other)
torch.Tensor.lerp(end, weight)
torch.Tensor.lerp_(end, weight)
torch.Tensor.less()
torch.Tensor.less_(other)
torch.Tensor.less_equal(other)
torch.Tensor.less_equal_(other)
torch.Tensor.lgamma()
torch.Tensor.lgamma_()
torch.Tensor.log()
torch.Tensor.log10()
torch.Tensor.log10_()
torch.Tensor.log1p()
torch.Tensor.log1p_()
torch.Tensor.log2()
torch.Tensor.log2_()
torch.Tensor.log_()
torch.Tensor.log_normal_(mean=1, std=2, *, generator=None)
torch.Tensor.logaddexp(other)
torch.Tensor.logaddexp2(other)
torch.Tensor.logcumsumexp(dim)
torch.Tensor.logdet()
torch.Tensor.logical_and()
torch.Tensor.logical_and_()
torch.Tensor.logical_not()
torch.Tensor.logical_not_()
torch.Tensor.logical_or()
torch.Tensor.logical_or_()
torch.Tensor.logical_xor()
torch.Tensor.logical_xor_()
torch.Tensor.logit()
torch.Tensor.logit_()
torch.Tensor.logsumexp(dim, keepdim=False)
torch.Tensor.long(memory_format=torch.preserve_format)
torch.Tensor.lstsq(self, other)
torch.Tensor.lt(other)
torch.Tensor.lt_(other)
torch.Tensor.lu(self, pivot=True, get_infos=False)
torch.Tensor.lu_solve(LU_data, LU_pivots)
torch.Tensor.map_(tensor, callable)
torch.Tensor.masked_fill(mask, value)
torch.Tensor.masked_fill_(mask, value)
torch.Tensor.masked_scatter(mask, tensor)
torch.Tensor.masked_scatter_(mask, source)
torch.Tensor.masked_select(mask)
torch.Tensor.matmul(tensor2)
torch.Tensor.matrix_exp()
torch.Tensor.matrix_power(n)
torch.Tensor.max(dim=None, keepdim=False)
torch.Tensor.maximum(other)
torch.Tensor.mean(dim=None, keepdim=False, *, dtype=None)
torch.Tensor.median(dim=None, keepdim=False)
torch.Tensor.min(dim=None, keepdim=False)
torch.Tensor.minimum(other)
torch.Tensor.mm(mat2)
torch.Tensor.mode(dim=None, keepdim=False)
torch.Tensor.moveaxis(source, destination)
torch.Tensor.movedim(source, destination)
torch.Tensor.msort()
torch.Tensor.mul(value)
torch.Tensor.mul_(value)
torch.Tensor.multinomial(num_samples, replacement=False, *, generator=None)
torch.Tensor.multiply(value)
torch.Tensor.multiply_(value)
torch.Tensor.mv(vec)
torch.Tensor.mvlgamma(p)
torch.Tensor.mvlgamma_(p)
torch.Tensor.nan_to_num(nan=0.0, posinf=None, neginf=None)
torch.Tensor.nan_to_num_(nan=0.0, posinf=None, neginf=None)
torch.Tensor.nanmean(dim=None, keepdim=False, *, dtype=None)
torch.Tensor.nanmedian(dim=None, keepdim=False)
torch.Tensor.nanquantile(q, dim=None, keepdim=False, *, interpolation='linear')
torch.Tensor.nansum(dim=None, keepdim=False, dtype=None)
torch.Tensor.narrow(dimension, start, length)
torch.Tensor.narrow_copy(dimension, start, length)
torch.Tensor.ndim()
torch.Tensor.ndimension()
torch.Tensor.ne(other)
torch.Tensor.ne_(other)
torch.Tensor.neg()
torch.Tensor.neg_()
torch.Tensor.negative()
torch.Tensor.negative_()
torch.Tensor.nelement()
torch.Tensor.new_empty(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)
torch.Tensor.new_full(size, fill_value, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)
torch.Tensor.new_ones(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)
torch.Tensor.new_tensor(data, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)
torch.Tensor.new_zeros(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False)
torch.Tensor.nextafter(other)
torch.Tensor.nextafter_(other)
torch.Tensor.nonzero()
torch.Tensor.norm(self, p: Union[float, str, NoneType] = 'fro', dim=None, keepdim=False, dtype=None)
torch.Tensor.normal_(mean=0, std=1, *, generator=None)
torch.Tensor.not_equal(other)
torch.Tensor.not_equal_(other)
torch.Tensor.numel()
torch.Tensor.numpy(*, force=False)
torch.Tensor.orgqr(input2)
torch.Tensor.ormqr(input2, input3, left=True, transpose=False)
torch.Tensor.outer(vec2)
torch.Tensor.permute(*dims)
torch.Tensor.pin_memory()
torch.Tensor.pinverse()
torch.Tensor.polygamma(n)
torch.Tensor.polygamma_(n)
torch.Tensor.positive()
torch.Tensor.pow(exponent)
torch.Tensor.pow_(exponent)
torch.Tensor.prod(dim=None, keepdim=False, dtype=None)
torch.Tensor.put_(index, source, accumulate=False)
torch.Tensor.q_per_channel_axis()
torch.Tensor.q_per_channel_scales()
torch.Tensor.q_per_channel_zero_points()
torch.Tensor.q_scale()
torch.Tensor.q_zero_point()
torch.Tensor.qscheme()
torch.Tensor.quantile(q, dim=None, keepdim=False, *, interpolation='linear')
torch.Tensor.rad2deg()
torch.Tensor.random_(from=0, to=None, *, generator=None)
torch.Tensor.ravel()
torch.Tensor.real()
torch.Tensor.reciprocal()
torch.Tensor.reciprocal_()
torch.Tensor.record_stream(stream)
torch.Tensor.register_hook(self, hook)
torch.Tensor.remainder(divisor)
torch.Tensor.remainder_(divisor)
torch.Tensor.renorm(p, dim, maxnorm)
torch.Tensor.renorm_(p, dim, maxnorm)
torch.Tensor.repeat(*sizes)
torch.Tensor.repeat_interleave(repeats, dim=None, *, output_size=None)
torch.Tensor.requires_grad()
torch.Tensor.requires_grad_(requires_grad=True)
torch.Tensor.reshape(*shape)
torch.Tensor.reshape_as(other)
torch.Tensor.resize_(*sizes, memory_format=torch.contiguous_format)
torch.Tensor.resize_as_(tensor, memory_format=torch.contiguous_format)
torch.Tensor.resolve_conj()
torch.Tensor.resolve_neg()
torch.Tensor.retain_grad()
torch.Tensor.retains_grad()
torch.Tensor.roll(shifts, dims)
torch.Tensor.rot90(k, dims)
torch.Tensor.round(decimals=0)
torch.Tensor.round_(decimals=0)
torch.Tensor.rsqrt()
torch.Tensor.rsqrt_()
torch.Tensor.scatter(dim, index, src)
torch.Tensor.scatter_(dim, index, src, *, reduce=None)
torch.Tensor.scatter_add(dim, index, src)
torch.Tensor.scatter_add_(dim, index, src)
torch.Tensor.select(dim, index)
torch.Tensor.set_(source=None, storage_offset=0, size=None, stride=None)
torch.Tensor.sgn()
torch.Tensor.sgn_()
torch.Tensor.share_memory_(self)
torch.Tensor.short(memory_format=torch.preserve_format)
torch.Tensor.sigmoid()
torch.Tensor.sigmoid_()
torch.Tensor.sign()
torch.Tensor.sign_()
torch.Tensor.signbit()
torch.Tensor.sin()
torch.Tensor.sin_()
torch.Tensor.sinc()
torch.Tensor.sinc_()
torch.Tensor.sinh()
torch.Tensor.sinh_()
torch.Tensor.size(dim=None)
torch.Tensor.slogdet()
torch.Tensor.smm(mat)
torch.Tensor.solve(self, other)
torch.Tensor.sort(dim=-1, descending=False)
torch.Tensor.sparse_dim()
torch.Tensor.sparse_mask(mask)
torch.Tensor.split(self, split_size, dim=0)
torch.Tensor.sqrt()
torch.Tensor.sqrt_()
torch.Tensor.square()
torch.Tensor.square_()
torch.Tensor.squeeze(dim=None)
torch.Tensor.squeeze_(dim=None)
torch.Tensor.sspaddmm(mat1, mat2, *, beta=1, alpha=1)
torch.Tensor.std(dim=None, *, correction=1, keepdim=False)
torch.Tensor.stft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)
torch.Tensor.storage()
torch.Tensor.storage_offset()
torch.Tensor.storage_type()
torch.Tensor.stride(dim)
torch.Tensor.sub(other, *, alpha=1)
torch.Tensor.sub_(other, *, alpha=1)
torch.Tensor.subtract(other, *, alpha=1)
torch.Tensor.subtract_(other, *, alpha=1)
torch.Tensor.sum(dim=None, keepdim=False, dtype=None)
torch.Tensor.sum_to_size(*size)
torch.Tensor.swapaxes(axis0, axis1)
torch.Tensor.swapdims(dim0, dim1)
torch.Tensor.symeig(self, eigenvectors=False)
torch.Tensor.t()
torch.Tensor.t_()
torch.Tensor.take(indices)
torch.Tensor.take_along_dim(indices, dim)
torch.Tensor.tan()
torch.Tensor.tan_()
torch.Tensor.tanh()
torch.Tensor.tanh_()
torch.Tensor.tensor_split(indices_or_sections, dim=0)
torch.Tensor.tile(dims)
torch.Tensor.to(*args, **kwargs)
torch.Tensor.to_mkldnn()
torch.Tensor.to_sparse(sparseDims)
torch.Tensor.tolist()
torch.Tensor.topk(k, dim=None, largest=True, sorted=True)
torch.Tensor.trace()
torch.Tensor.transpose(dim0, dim1)
torch.Tensor.transpose_(dim0, dim1)
torch.Tensor.triangular_solve(A, upper=True, transpose=False, unitriangular=False)
torch.Tensor.tril(diagonal=0)
torch.Tensor.tril_(diagonal=0)
torch.Tensor.triu(diagonal=0)
torch.Tensor.triu_(diagonal=0)
torch.Tensor.true_divide(value)
torch.Tensor.true_divide_(value)
torch.Tensor.trunc()
torch.Tensor.trunc_()
torch.Tensor.type(dtype=None, non_blocking=False, **kwargs)
torch.Tensor.type_as(tensor)
torch.Tensor.unbind(dim=0)
torch.Tensor.unfold(dimension, size, step)
torch.Tensor.uniform_(from=0, to=1, *, generator=None)
torch.Tensor.unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None)
torch.Tensor.unique_consecutive(self, return_inverse=False, return_counts=False, dim=None)
torch.Tensor.unsqueeze(dim)
torch.Tensor.unsqueeze_(dim)
torch.Tensor.values()
torch.Tensor.var(dim=None, *, correction=1, keepdim=False)
torch.Tensor.vdot(other)
torch.Tensor.view(*shape)
torch.Tensor.view_as(other)
torch.Tensor.vsplit(split_size_or_sections)
torch.Tensor.xlogy(other)
torch.Tensor.xlogy_(other)
torch.Tensor.zero_()
torch._assert(condition, message)
torch.abs(input, *, out=None)
torch.absolute(input, *, out=None)
torch.acos(input, *, out=None)
torch.acosh(input, *, out=None)
torch.add(input, other, *, alpha=1, out=None)
torch.addcdiv(input, tensor1, tensor2, *, value=1, out=None)
torch.addcmul(input, tensor1, tensor2, *, value=1, out=None)
torch.addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None)
torch.addmv(input, mat, vec, *, beta=1, alpha=1, out=None)
torch.addr(input, vec1, vec2, *, beta=1, alpha=1, out=None)
torch.all(input)
torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)
torch.amax(input, dim, keepdim=False, *, out=None)
torch.amin(input, dim, keepdim=False, *, out=None)
torch.aminmax(input, *, dim=None, keepdim=False, out=None)
torch.angle(input, *, out=None)
torch.any(input)
torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.arccos(input, *, out=None)
torch.arccosh(input, *, out=None)
torch.arcsin(input, *, out=None)
torch.arcsinh(input, *, out=None)
torch.arctan(input, *, out=None)
torch.arctanh(input, *, out=None)
torch.are_deterministic_algorithms_enabled()
torch.argmax(input)
torch.argmin(input, dim=None, keepdim=False)
torch.argsort(input, dim=-1, descending=False, stable=False)
torch.as_strided(input, size, stride, storage_offset=None)
torch.as_tensor(data, dtype=None, device=None)
torch.asin(input, *, out=None)
torch.asinh(input, *, out=None)
torch.atan(input, *, out=None)
torch.atan2(input, other, *, out=None)
torch.atanh(input, *, out=None)
torch.atleast_1d(*tensors)
torch.atleast_2d(*tensors)
torch.atleast_3d(*tensors)
torch.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None)
torch.bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.bernoulli(input, *, generator=None, out=None)
torch.bincount(input, weights=None, minlength=0)
torch.bitwise_and(input, other, *, out=None)
torch.bitwise_left_shift(input, other, *, out=None)
torch.bitwise_not(input, *, out=None)
torch.bitwise_or(input, other, *, out=None)
torch.bitwise_right_shift(input, other, *, out=None)
torch.bitwise_xor(input, other, *, out=None)
torch.blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.block_diag(*tensors)
torch.bmm(input, mat2, *, out=None)
torch.broadcast_shapes(*shapes)
torch.broadcast_tensors(*tensors)
torch.broadcast_to(input, shape)
torch.bucketize(input, boundaries, *, out_int32=False, right=False, out=None)
torch.can_cast(from_, to)
torch.cartesian_prod(*tensors: torch.Tensor)
torch.cat(tensors, dim=0, *, out=None)
torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')
torch.ceil(input, *, out=None)
torch.chain_matmul(*matrices, out=None)
torch.cholesky(input, upper=False, *, out=None)
torch.cholesky_inverse(L, upper=False, *, out=None)
torch.cholesky_solve(B, L, upper=False, *, out=None)
torch.chunk(input, chunks, dim=0)
torch.clamp(input, min=None, max=None, *, out=None)
torch.clip(input, min=None, max=None, *, out=None)
torch.clone(input, *, memory_format=torch.preserve_format)
torch.column_stack(tensors, *, out=None)
torch.combinations(input, r=2, with_replacement=False)
torch.compiled_with_cxx11_abi()
torch.complex(real, imag, *, out=None)
torch.concat(tensors, dim=0, *, out=None)
torch.conj(input)
torch.conj_physical(input, *, out=None)
torch.copysign(input, other, *, out=None)
torch.corrcoef(input)
torch.cos(input, *, out=None)
torch.cosh(input, *, out=None)
torch.count_nonzero(input, dim=None)
torch.cov(input, *, correction=1, fweights=None, aweights=None)
torch.cross(input, other, dim=None, *, out=None)
torch.cummax(input, dim, *, out=None)
torch.cummin(input, dim, *, out=None)
torch.cumprod(input, dim, *, dtype=None, out=None)
torch.cumsum(input, dim, *, dtype=None, out=None)
torch.cumulative_trapezoid(y, x=None, *, dx=None, dim=-1)
torch.deg2rad(input, *, out=None)
torch.dequantize(tensor)
torch.det(input)
torch.diag(input, diagonal=0, *, out=None)
torch.diag_embed(input, offset=0, dim1=-2, dim2=-1)
torch.diagflat(input, offset=0)
torch.diagonal(input, offset=0, dim1=0, dim2=1)
torch.diff(input, n=1, dim=-1, prepend=None, append=None)
torch.digamma(input, *, out=None)
torch.dist(input, other, p=2)
torch.distributed.Backend(name: str)
torch.distributed.HashStore(torch.distributed.distributed_c10d.Store)
torch.distributed.Store(pybind11_builtins.pybind11_object)
torch.distributed.algorithms.JoinHook(builtins.object)
torch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False)
torch.distributed.init_process_group(backend: Optional[str] = None, init_method: Optional[str] = None, timeout: Optional[datetime.timedelta] = None, world_size: int = -1, rank: int = -1, store: Optional[torch.distributed.distributed_c10d.Store] = None, group_name: str = '', pg_options: Optional[Any] = None, device_id: Optional[torch.device] = None)
torch.distributed.is_available()
torch.distributed.is_initialized()
torch.distributed.is_mpi_available()
torch.distributed.is_nccl_available()
torch.distributed.is_torchelastic_launched()
torch.distributions.constraints.Constraint(builtins.object)
torch.distributions.exp_family.ExponentialFamily(batch_shape: torch.Size = torch.Size([]), event_shape: torch.Size = torch.Size([]), validate_args: Optional[bool] = None)
torch.distributions.kl.kl_divergence(p: torch.distributions.distribution.Distribution, q: torch.distributions.distribution.Distribution)
torch.distributions.transforms.AbsTransform(cache_size=0)
torch.distributions.transforms.CorrCholeskyTransform(cache_size=0)
torch.distributions.transforms.ExpTransform(cache_size=0)
torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)
torch.distributions.transforms.LowerCholeskyTransform(cache_size=0)
torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0)
torch.distributions.transforms.SigmoidTransform(cache_size=0)
torch.distributions.transforms.SoftmaxTransform(cache_size=0)
torch.distributions.transforms.StickBreakingTransform(cache_size=0)
torch.distributions.transforms.TanhTransform(cache_size=0)
torch.div(input, other, *, rounding_mode=None, out=None)
torch.divide(input, other, *, rounding_mode=None, out=None)
torch.dot(input, tensor, *, out=None)
torch.dsplit(input, indices_or_sections)
torch.dstack(tensors, *, out=None)
torch.eig(self: torch.Tensor, eigenvectors: bool = False, *, e=None, v=None)
torch.einsum(equation, *operands)
torch.empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format)
torch.empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)
torch.empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False)
torch.enable_grad(orig_func=None)
torch.eq(input, other, *, out=None)
torch.equal(input, other)
torch.erf(input, *, out=None)
torch.erfc(input, *, out=None)
torch.erfinv(input, *, out=None)
torch.exp(input, *, out=None)
torch.exp2(input, *, out=None)
torch.expm1(input, *, out=None)
torch.eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max)
torch.fft.fft(input, n=None, dim=-1, norm=None, *, out=None)
torch.fft.fft2(input, s=None, dim=(-2, -1), norm=None, *, out=None)
torch.fft.fftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.fft.fftn(input, s=None, dim=None, norm=None, *, out=None)
torch.fft.fftshift(input, dim=None)
torch.fft.hfft(input, n=None, dim=-1, norm=None, *, out=None)
torch.fft.ifft(input, n=None, dim=-1, norm=None, *, out=None)
torch.fft.ifft2(input, s=None, dim=(-2, -1), norm=None, *, out=None)
torch.fft.ifftn(input, s=None, dim=None, norm=None, *, out=None)
torch.fft.ifftshift(input, dim=None)
torch.fft.ihfft(input, n=None, dim=-1, norm=None, *, out=None)
torch.fft.irfft(input, n=None, dim=-1, norm=None, *, out=None)
torch.fft.irfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None)
torch.fft.irfftn(input, s=None, dim=None, norm=None, *, out=None)
torch.fft.rfft(input, n=None, dim=-1, norm=None, *, out=None)
torch.fft.rfft2(input, s=None, dim=(-2, -1), norm=None, *, out=None)
torch.fft.rfftfreq(n, d=1.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.fft.rfftn(input, s=None, dim=None, norm=None, *, out=None)
torch.fix(input, *, out=None)
torch.flatten(input, start_dim=0, end_dim=-1)
torch.flip(input, dims)
torch.fliplr(input)
torch.flipud(input)
torch.float_power(input, exponent, *, out=None)
torch.floor(input, *, out=None)
torch.floor_divide(input, other, *, out=None)
torch.fmax(input, other, *, out=None)
torch.fmin(input, other, *, out=None)
torch.fmod(input, other, *, out=None)
torch.frac(input, *, out=None)
torch.frexp(input, *, out=None)
torch.from_numpy(ndarray)
torch.frombuffer(buffer, *, dtype, count=-1, offset=0, requires_grad=False)
torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.full_like(input, fill_value, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)
torch.futures.Future(*, devices: 'Optional[List[Union[int, str, torch.device]]]' = None)
torch.futures.wait_all(futures: 'List[Future]')
torch.fx.Graph(owning_module: Optional[ForwardRef('GraphModule')] = None, tracer_cls: Optional[Type[ForwardRef('Tracer')]] = None, tracer_extras: Optional[Dict[str, Any]] = None)
torch.fx.symbolic_trace(root: Union[torch.nn.modules.module.Module, Callable[..., Any]], concrete_args: Optional[Dict[str, Any]] = None)
torch.gather(input, dim, index, *, sparse_grad=False, out=None)
torch.gcd(input, other, *, out=None)
torch.ge(input, other, *, out=None)
torch.geqrf(input, *, out=None)
torch.ger(input, vec2, *, out=None)
torch.get_default_dtype()
torch.get_num_interop_threads()
torch.get_num_threads()
torch.get_rng_state()
torch.gradient(input, *, spacing=1, dim=None, edge_order=1)
torch.greater(input, other, *, out=None)
torch.greater_equal(input, other, *, out=None)
torch.gt(input, other, *, out=None)
torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.hann_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.heaviside(input, values, *, out=None)
torch.histc(input, bins=100, min=0, max=0, *, out=None)
torch.histogram(input, bins, *, range=None, weight=None, density=False, out=None)
torch.hsplit(input, indices_or_sections)
torch.hstack(tensors, *, out=None)
torch.hub.get_dir()
torch.hub.help(github, model, force_reload=False, skip_validation=False, trust_repo=None)
torch.hub.load(repo_or_dir, model, *args, source='github', trust_repo=None, force_reload=False, verbose=True, skip_validation=False, **kwargs)
torch.hub.load_state_dict_from_url(url: str, model_dir: Optional[str] = None, map_location: Union[Callable[[torch.types.Storage, str], torch.types.Storage], torch.device, str, Dict[str, str], NoneType] = None, progress: bool = True, check_hash: bool = False, file_name: Optional[str] = None, weights_only: bool = False)
torch.hub.set_dir(d)
torch.hypot(input, other, *, out=None)
torch.i0(input, *, out=None)
torch.igamma(input, other, *, out=None)
torch.igammac(input, other, *, out=None)
torch.imag(input)
torch.index_select(input, dim, index, *, out=None)
torch.inference_mode(mode=True)
torch.initial_seed()
torch.inner(input, other, *, out=None)
torch.inverse(input, *, out=None)
torch.is_complex(input)
torch.is_conj(input)
torch.is_floating_point(input)
torch.is_grad_enabled()
torch.is_inference_mode_enabled()
torch.is_storage(obj)
torch.is_tensor(obj)
torch.is_warn_always_enabled()
torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)
torch.isfinite(input)
torch.isin(elements, test_elements, *, assume_unique=False, invert=False)
torch.isinf(input)
torch.isnan(input)
torch.isneginf(input, *, out=None)
torch.isposinf(input, *, out=None)
torch.isreal(input)
torch.jit.annotate(the_type, the_value)
torch.jit.fork(func, *args, **kwargs)
torch.jit.isinstance(obj, target_type)
torch.jit.trace(func, example_inputs=None, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x109c3cfb0>, example_kwarg_inputs=None, _store_inputs=True)
torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object at 0x109c3cfb0>, example_inputs_is_kwarg=False, _store_inputs=True)
torch.jit.unused(fn)
torch.jit.wait(future)
torch.kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.kron(input, other, *, out=None)
torch.kthvalue(input, k, dim=None, keepdim=False, *, out=None)
torch.lcm(input, other, *, out=None)
torch.ldexp(input, other, *, out=None)
torch.le(input, other, *, out=None)
torch.lerp(input, end, weight, *, out=None)
torch.less(input, other, *, out=None)
torch.less_equal(input, other, *, out=None)
torch.lgamma(input, *, out=None)
torch.linalg.cholesky(A, *, upper=False, out=None)
torch.linalg.cholesky_ex(A, *, upper=False, check_errors=False, out=None)
torch.linalg.cond(A, p=None, *, out=None)
torch.linalg.det(A, *, out=None)
torch.linalg.eig(A, *, out=None)
torch.linalg.eigh(A, UPLO='L', *, out=None)
torch.linalg.eigvals(A, *, out=None)
torch.linalg.eigvalsh(A, UPLO='L', *, out=None)
torch.linalg.householder_product(A, tau, *, out=None)
torch.linalg.inv(A, *, out=None)
torch.linalg.inv_ex(A, *, check_errors=False, out=None)
torch.linalg.lstsq(A, B, rcond=None, *, driver=None)
torch.linalg.matmul(input, other, *, out=None)
torch.linalg.matrix_norm(A, ord='fro', dim=(-2, -1), keepdim=False, *, dtype=None, out=None)
torch.linalg.matrix_power(A, n, *, out=None)
torch.linalg.matrix_rank(A, *, atol=None, rtol=None, hermitian=False, out=None)
torch.linalg.multi_dot(tensors, *, out=None)
torch.linalg.norm(A, ord=None, dim=None, keepdim=False, *, out=None, dtype=None)
torch.linalg.pinv(A, *, atol=None, rtol=None, hermitian=False, out=None)
torch.linalg.qr(A, mode='reduced', *, out=None)
torch.linalg.slogdet(A, *, out=None)
torch.linalg.solve(A, B, *, left=True, out=None)
torch.linalg.svd(A, full_matrices=True, *, driver=None, out=None)
torch.linalg.svdvals(A, *, driver=None, out=None)
torch.linalg.vector_norm(x, ord=2, dim=None, keepdim=False, *, dtype=None, out=None)
torch.linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.lobpcg(A: torch.Tensor, k: Optional[int] = None, B: Optional[torch.Tensor] = None, X: Optional[torch.Tensor] = None, n: Optional[int] = None, iK: Optional[torch.Tensor] = None, niter: Optional[int] = None, tol: Optional[float] = None, largest: Optional[bool] = None, method: Optional[str] = None, tracker: None = None, ortho_iparams: Optional[Dict[str, int]] = None, ortho_fparams: Optional[Dict[str, float]] = None, ortho_bparams: Optional[Dict[str, bool]] = None)
torch.log(input, *, out=None)
torch.log10(input, *, out=None)
torch.log1p(input, *, out=None)
torch.log2(input, *, out=None)
torch.logaddexp(input, other, *, out=None)
torch.logaddexp2(input, other, *, out=None)
torch.logcumsumexp(input, dim, *, out=None)
torch.logical_and(input, other, *, out=None)
torch.logical_not(input, *, out=None)
torch.logical_or(input, other, *, out=None)
torch.logical_xor(input, other, *, out=None)
torch.logit(input, eps=None, *, out=None)
torch.logspace(start, end, steps, base=10.0, *,          out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.logsumexp(input, dim, keepdim=False, *, out=None)
torch.lstsq(input: torch.Tensor, A: torch.Tensor, *, out=None)
torch.lt(input, other, *, out=None)
torch.lu(*args, **kwargs)
torch.lu_solve(b, LU_data, LU_pivots, *, out=None)
torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, *, out=None)
torch.masked_select(input, mask, *, out=None)
torch.matmul(input, other, *, out=None)
torch.matrix_exp(A)
torch.matrix_power(input, n, *, out=None)
torch.matrix_rank(input, tol=None, symmetric=False, *, out=None)
torch.max(input)
torch.maximum(input, other, *, out=None)
torch.mean(input, *, dtype=None)
torch.median(input)
torch.meshgrid(*tensors, indexing: Optional[str] = None)
torch.min(input)
torch.minimum(input, other, *, out=None)
torch.mm(input, mat2, *, out=None)
torch.mode(input, dim=-1, keepdim=False, *, out=None)
torch.moveaxis(input, source, destination)
torch.movedim(input, source, destination)
torch.msort(input, *, out=None)
torch.mul(input, other, *, out=None)
torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None)
torch.multiply(input, other, *, out=None)
torch.mv(input, vec, *, out=None)
torch.mvlgamma(input, p, *, out=None)
torch.nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None)
torch.nanmean(input, dim=None, keepdim=False, *, dtype=None, out=None)
torch.nanmedian(input)
torch.nanquantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None)
torch.nansum(input, *, dtype=None)
torch.narrow(input, dim, start, length)
torch.ne(input, other, *, out=None)
torch.neg(input, *, out=None)
torch.negative(input, *, out=None)
torch.nextafter(input, other, *, out=None)
torch.nn.AdaptiveAvgPool1d(output_size: Union[int, NoneType, Tuple[Optional[int], ...]])
torch.nn.AdaptiveAvgPool2d(output_size: Union[int, NoneType, Tuple[Optional[int], ...]])
torch.nn.AdaptiveAvgPool3d(output_size: Union[int, NoneType, Tuple[Optional[int], ...]])
torch.nn.AdaptiveLogSoftmaxWithLoss(in_features: int, n_classes: int, cutoffs: Sequence[int], div_value: float = 4.0, head_bias: bool = False, device=None, dtype=None)
torch.nn.AdaptiveMaxPool1d(output_size: Union[int, NoneType, Tuple[Optional[int], ...]], return_indices: bool = False)
torch.nn.AdaptiveMaxPool2d(output_size: Union[int, NoneType, Tuple[Optional[int], ...]], return_indices: bool = False)
torch.nn.AdaptiveMaxPool3d(output_size: Union[int, NoneType, Tuple[Optional[int], ...]], return_indices: bool = False)
torch.nn.AlphaDropout(p: float = 0.5, inplace: bool = False)
torch.nn.AvgPool1d(kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = None, padding: Union[int, Tuple[int]] = 0, ceil_mode: bool = False, count_include_pad: bool = True)
torch.nn.AvgPool2d(kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int], NoneType] = None, padding: Union[int, Tuple[int, int]] = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None)
torch.nn.AvgPool3d(kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int], NoneType] = None, padding: Union[int, Tuple[int, int, int]] = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Optional[int] = None)
torch.nn.BCELoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.BCEWithLogitsLoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean', pos_weight: Optional[torch.Tensor] = None)
torch.nn.BatchNorm1d(num_features: int, eps: float = 1e-05, momentum: Optional[float] = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None)
torch.nn.BatchNorm2d(num_features: int, eps: float = 1e-05, momentum: Optional[float] = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None)
torch.nn.Bilinear(in1_features: int, in2_features: int, out_features: int, bias: bool = True, device=None, dtype=None)
torch.nn.CELU(alpha: float = 1.0, inplace: bool = False)
torch.nn.CTCLoss(blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False)
torch.nn.ChannelShuffle(groups: int)
torch.nn.ConstantPad1d(padding: Union[int, Tuple[int, int]], value: float)
torch.nn.ConstantPad2d(padding: Union[int, Tuple[int, int, int, int]], value: float)
torch.nn.ConstantPad3d(padding: Union[int, Tuple[int, int, int, int, int, int]], value: float)
torch.nn.Conv1d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[str, int, Tuple[int]] = 0, dilation: Union[int, Tuple[int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[str, int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.Conv3d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[str, int, Tuple[int, int, int]] = 0, dilation: Union[int, Tuple[int, int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.ConvTranspose1d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[int, Tuple[int]] = 0, output_padding: Union[int, Tuple[int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int]] = 1, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.ConvTranspose2d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, output_padding: Union[int, Tuple[int, int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int, int]] = 1, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.ConvTranspose3d(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[int, Tuple[int, int, int]] = 0, output_padding: Union[int, Tuple[int, int, int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int, int, int]] = 1, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.CosineEmbeddingLoss(margin: float = 0.0, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.CosineSimilarity(dim: int = 1, eps: float = 1e-08)
torch.nn.CrossEntropyLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0)
torch.nn.Dropout(p: float = 0.5, inplace: bool = False)
torch.nn.Dropout2d(p: float = 0.5, inplace: bool = False)
torch.nn.Dropout3d(p: float = 0.5, inplace: bool = False)
torch.nn.ELU(alpha: float = 1.0, inplace: bool = False)
torch.nn.Embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False, _weight: Optional[torch.Tensor] = None, _freeze: bool = False, device=None, dtype=None)
torch.nn.EmbeddingBag(num_embeddings: int, embedding_dim: int, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, _weight: Optional[torch.Tensor] = None, include_last_offset: bool = False, padding_idx: Optional[int] = None, device=None, dtype=None)
torch.nn.FeatureAlphaDropout(p: float = 0.5, inplace: bool = False)
torch.nn.Flatten(start_dim: int = 1, end_dim: int = -1)
torch.nn.FractionalMaxPool2d(kernel_size: Union[int, Tuple[int, int]], output_size: Union[int, Tuple[int, int], NoneType] = None, output_ratio: Union[float, Tuple[float, float], NoneType] = None, return_indices: bool = False, _random_samples=None)
torch.nn.FractionalMaxPool3d(kernel_size: Union[int, Tuple[int, int, int]], output_size: Union[int, Tuple[int, int, int], NoneType] = None, output_ratio: Union[float, Tuple[float, float, float], NoneType] = None, return_indices: bool = False, _random_samples=None)
torch.nn.GELU(approximate: str = 'none')
torch.nn.GLU(dim: int = -1)
torch.nn.GRU(*args, **kwargs)
torch.nn.GRUCell(input_size: int, hidden_size: int, bias: bool = True, device=None, dtype=None)
torch.nn.GroupNorm(num_groups: int, num_channels: int, eps: float = 1e-05, affine: bool = True, device=None, dtype=None)
torch.nn.Hardshrink(lambd: float = 0.5)
torch.nn.Hardsigmoid(inplace: bool = False)
torch.nn.Hardswish(inplace: bool = False)
torch.nn.Hardtanh(min_val: float = -1.0, max_val: float = 1.0, inplace: bool = False, min_value: Optional[float] = None, max_value: Optional[float] = None)
torch.nn.HingeEmbeddingLoss(margin: float = 1.0, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.HuberLoss(reduction: str = 'mean', delta: float = 1.0)
torch.nn.Identity(*args: Any, **kwargs: Any)
torch.nn.InstanceNorm1d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None)
torch.nn.InstanceNorm2d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None)
torch.nn.InstanceNorm3d(num_features: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False, device=None, dtype=None)
torch.nn.KLDivLoss(size_average=None, reduce=None, reduction: str = 'mean', log_target: bool = False)
torch.nn.L1Loss(size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.LPPool1d(norm_type: float, kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, ceil_mode: bool = False)
torch.nn.LPPool2d(norm_type: float, kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, ceil_mode: bool = False)
torch.nn.LSTM(*args, **kwargs)
torch.nn.LSTMCell(input_size: int, hidden_size: int, bias: bool = True, device=None, dtype=None)
torch.nn.LayerNorm(normalized_shape: Union[int, List[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True, bias: bool = True, device=None, dtype=None)
torch.nn.LazyBatchNorm1d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
torch.nn.LazyBatchNorm3d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
torch.nn.LazyConv1d(out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[int, Tuple[int]] = 0, dilation: Union[int, Tuple[int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.LazyConv2d(out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.LazyConv3d(out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[int, Tuple[int, int, int]] = 0, dilation: Union[int, Tuple[int, int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.LazyConvTranspose1d(out_channels: int, kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int]] = 1, padding: Union[int, Tuple[int]] = 0, output_padding: Union[int, Tuple[int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int]] = 1, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.LazyConvTranspose2d(out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, output_padding: Union[int, Tuple[int, int]] = 0, groups: int = 1, bias: bool = True, dilation: int = 1, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.LazyConvTranspose3d(out_channels: int, kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int]] = 1, padding: Union[int, Tuple[int, int, int]] = 0, output_padding: Union[int, Tuple[int, int, int]] = 0, groups: int = 1, bias: bool = True, dilation: Union[int, Tuple[int, int, int]] = 1, padding_mode: str = 'zeros', device=None, dtype=None)
torch.nn.LazyInstanceNorm1d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
torch.nn.LazyInstanceNorm2d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
torch.nn.LazyInstanceNorm3d(eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)
torch.nn.LazyLinear(out_features: int, bias: bool = True, device=None, dtype=None)
torch.nn.LeakyReLU(negative_slope: float = 0.01, inplace: bool = False)
torch.nn.Linear(in_features: int, out_features: int, bias: bool = True, device=None, dtype=None)
torch.nn.LocalResponseNorm(size: int, alpha: float = 0.0001, beta: float = 0.75, k: float = 1.0)
torch.nn.LogSigmoid(*args, **kwargs)
torch.nn.LogSoftmax(dim: Optional[int] = None)
torch.nn.MSELoss(size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.MarginRankingLoss(margin: float = 0.0, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.MaxPool1d(kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0, dilation: Union[int, Tuple[int, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)
torch.nn.MaxPool2d(kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0, dilation: Union[int, Tuple[int, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)
torch.nn.MaxPool3d(kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0, dilation: Union[int, Tuple[int, ...]] = 1, return_indices: bool = False, ceil_mode: bool = False)
torch.nn.MaxUnpool1d(kernel_size: Union[int, Tuple[int]], stride: Union[int, Tuple[int], NoneType] = None, padding: Union[int, Tuple[int]] = 0)
torch.nn.MaxUnpool2d(kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int], NoneType] = None, padding: Union[int, Tuple[int, int]] = 0)
torch.nn.MaxUnpool3d(kernel_size: Union[int, Tuple[int, int, int]], stride: Union[int, Tuple[int, int, int], NoneType] = None, padding: Union[int, Tuple[int, int, int]] = 0)
torch.nn.Mish(inplace: bool = False)
torch.nn.ModuleDict(modules: Optional[Mapping[str, torch.nn.modules.module.Module]] = None)
torch.nn.ModuleList(modules: Optional[Iterable[torch.nn.modules.module.Module]] = None)
torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.MultiLabelSoftMarginLoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.MultiMarginLoss(p: int = 1, margin: float = 1.0, weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None)
torch.nn.NLLLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = 'mean')
torch.nn.PReLU(num_parameters: int = 1, init: float = 0.25, device=None, dtype=None)
torch.nn.PairwiseDistance(p: float = 2.0, eps: float = 1e-06, keepdim: bool = False)
torch.nn.ParameterDict(parameters: Any = None)
torch.nn.ParameterList(values: Optional[Iterable[Any]] = None)
torch.nn.PixelShuffle(upscale_factor: int)
torch.nn.PixelUnshuffle(downscale_factor: int)
torch.nn.PoissonNLLLoss(log_input: bool = True, full: bool = False, size_average=None, eps: float = 1e-08, reduce=None, reduction: str = 'mean')
torch.nn.RNNBase(mode: str, input_size: int, hidden_size: int, num_layers: int = 1, bias: bool = True, batch_first: bool = False, dropout: float = 0.0, bidirectional: bool = False, proj_size: int = 0, device=None, dtype=None)
torch.nn.RNNCell(input_size: int, hidden_size: int, bias: bool = True, nonlinearity: str = 'tanh', device=None, dtype=None)
torch.nn.RReLU(lower: float = 0.125, upper: float = 0.3333333333333333, inplace: bool = False)
torch.nn.ReLU(inplace: bool = False)
torch.nn.ReLU6(inplace: bool = False)
torch.nn.ReflectionPad1d(padding: Union[int, Tuple[int, int]])
torch.nn.ReflectionPad2d(padding: Union[int, Tuple[int, int, int, int]])
torch.nn.ReflectionPad3d(padding: Union[int, Tuple[int, int, int, int, int, int]])
torch.nn.ReplicationPad1d(padding: Union[int, Tuple[int, int]])
torch.nn.ReplicationPad2d(padding: Union[int, Tuple[int, int, int, int]])
torch.nn.ReplicationPad3d(padding: Union[int, Tuple[int, int, int, int, int, int]])
torch.nn.SELU(inplace: bool = False)
torch.nn.Sequential(*args)
torch.nn.SiLU(inplace: bool = False)
torch.nn.Sigmoid(*args, **kwargs)
torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction: str = 'mean', beta: float = 1.0)
torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.Softmax(dim: Optional[int] = None)
torch.nn.Softmax2d(*args, **kwargs)
torch.nn.Softmin(dim: Optional[int] = None)
torch.nn.Softplus(beta: float = 1.0, threshold: float = 20.0)
torch.nn.Softshrink(lambd: float = 0.5)
torch.nn.Softsign(*args, **kwargs)
torch.nn.SyncBatchNorm(num_features: int, eps: float = 1e-05, momentum: Optional[float] = 0.1, affine: bool = True, track_running_stats: bool = True, process_group: Optional[Any] = None, device=None, dtype=None)
torch.nn.Tanh(*args, **kwargs)
torch.nn.Tanhshrink(*args, **kwargs)
torch.nn.Threshold(threshold: float, value: float, inplace: bool = False)
torch.nn.TransformerDecoder(decoder_layer: 'TransformerDecoderLayer', num_layers: int, norm: Optional[torch.nn.modules.module.Module] = None)
torch.nn.TransformerDecoderLayer(d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[torch.Tensor], torch.Tensor]] = <function relu at 0x109e39440>, layer_norm_eps: float = 1e-05, batch_first: bool = False, norm_first: bool = False, bias: bool = True, device=None, dtype=None)
torch.nn.TransformerEncoder(encoder_layer: 'TransformerEncoderLayer', num_layers: int, norm: Optional[torch.nn.modules.module.Module] = None, enable_nested_tensor: bool = True, mask_check: bool = True)
torch.nn.TransformerEncoderLayer(d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1, activation: Union[str, Callable[[torch.Tensor], torch.Tensor]] = <function relu at 0x109e39440>, layer_norm_eps: float = 1e-05, batch_first: bool = False, norm_first: bool = False, bias: bool = True, device=None, dtype=None)
torch.nn.TripletMarginLoss(margin: float = 1.0, p: float = 2.0, eps: float = 1e-06, swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean')
torch.nn.TripletMarginWithDistanceLoss(*, distance_function: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean')
torch.nn.Unflatten(dim: Union[int, str], unflattened_size: Union[torch.Size, List[int], Tuple[int, ...], Tuple[Tuple[str, int]]])
torch.nn.Unfold(kernel_size: Union[int, Tuple[int, ...]], dilation: Union[int, Tuple[int, ...]] = 1, padding: Union[int, Tuple[int, ...]] = 0, stride: Union[int, Tuple[int, ...]] = 1)
torch.nn.Upsample(size: Union[int, Tuple[int, ...], NoneType] = None, scale_factor: Union[float, Tuple[float, ...], NoneType] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None)
torch.nn.UpsamplingBilinear2d(size: Union[int, Tuple[int, int], NoneType] = None, scale_factor: Union[float, Tuple[float, float], NoneType] = None)
torch.nn.UpsamplingNearest2d(size: Union[int, Tuple[int, int], NoneType] = None, scale_factor: Union[float, Tuple[float, float], NoneType] = None)
torch.nn.ZeroPad2d(padding: Union[int, Tuple[int, int, int, int]])
torch.nn.functional.adaptive_avg_pool1d(input, output_size)
torch.nn.functional.adaptive_avg_pool2d(input: torch.Tensor, output_size: None)
torch.nn.functional.adaptive_avg_pool3d(input: torch.Tensor, output_size: None)
torch.nn.functional.adaptive_max_pool1d(input, output_size, return_indices=False)
torch.nn.functional.adaptive_max_pool2d(input, output_size, return_indices=False)
torch.nn.functional.adaptive_max_pool3d(input, output_size, return_indices=False)
torch.nn.functional.affine_grid(theta: torch.Tensor, size: List[int], align_corners: Optional[bool] = None)
torch.nn.functional.alpha_dropout(input: torch.Tensor, p: float = 0.5, training: bool = False, inplace: bool = False)
torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)
torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)
torch.nn.functional.batch_norm(input: torch.Tensor, running_mean: Optional[torch.Tensor], running_var: Optional[torch.Tensor], weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, training: bool = False, momentum: float = 0.1, eps: float = 1e-05)
torch.nn.functional.bilinear(input1, input2, weight, bias=None)
torch.nn.functional.binary_cross_entropy(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean')
torch.nn.functional.binary_cross_entropy_with_logits(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', pos_weight: Optional[torch.Tensor] = None)
torch.nn.functional.celu(input, alpha=1., inplace=False)
torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)
torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)
torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)
torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)
torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1)
torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8)
torch.nn.functional.cross_entropy(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, ignore_index: int = -100, reduce: Optional[bool] = None, reduction: str = 'mean', label_smoothing: float = 0.0)
torch.nn.functional.ctc_loss(log_probs: torch.Tensor, targets: torch.Tensor, input_lengths: torch.Tensor, target_lengths: torch.Tensor, blank: int = 0, reduction: str = 'mean', zero_infinity: bool = False)
torch.nn.functional.dropout(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False)
torch.nn.functional.dropout2d(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False)
torch.nn.functional.dropout3d(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False)
torch.nn.functional.elu(input: torch.Tensor, alpha: float = 1.0, inplace: bool = False)
torch.nn.functional.elu_(input, alpha=1.)
torch.nn.functional.embedding(input: torch.Tensor, weight: torch.Tensor, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False)
torch.nn.functional.embedding_bag(input: torch.Tensor, weight: torch.Tensor, offsets: Optional[torch.Tensor] = None, max_norm: Optional[float] = None, norm_type: float = 2, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, per_sample_weights: Optional[torch.Tensor] = None, include_last_offset: bool = False, padding_idx: Optional[int] = None)
torch.nn.functional.feature_alpha_dropout(input: torch.Tensor, p: float = 0.5, training: bool = False, inplace: bool = False)
torch.nn.functional.fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)
torch.nn.functional.fractional_max_pool3d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)
torch.nn.functional.gaussian_nll_loss(input: torch.Tensor, target: torch.Tensor, var: torch.Tensor, full: bool = False, eps: float = 1e-06, reduction: str = 'mean')
torch.nn.functional.gelu(input, approximate = 'none')
torch.nn.functional.glu(input, dim=-1)
torch.nn.functional.group_norm(input: torch.Tensor, num_groups: int, weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, eps: float = 1e-05)
torch.nn.functional.gumbel_softmax(logits: torch.Tensor, tau: float = 1, hard: bool = False, eps: float = 1e-10, dim: int = -1)
torch.nn.functional.hardshrink(input, lambd=0.5)
torch.nn.functional.hardsigmoid(input: torch.Tensor, inplace: bool = False)
torch.nn.functional.hardswish(input: torch.Tensor, inplace: bool = False)
torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False)
torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.)
torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.huber_loss(input: torch.Tensor, target: torch.Tensor, reduction: str = 'mean', delta: float = 1.0)
torch.nn.functional.instance_norm(input: torch.Tensor, running_mean: Optional[torch.Tensor] = None, running_var: Optional[torch.Tensor] = None, weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, use_input_stats: bool = True, momentum: float = 0.1, eps: float = 1e-05)
torch.nn.functional.interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False)
torch.nn.functional.kl_div(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', log_target: bool = False)
torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.layer_norm(input: torch.Tensor, normalized_shape: List[int], weight: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, eps: float = 1e-05)
torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False)
torch.nn.functional.leaky_relu_(input, negative_slope=0.01)
torch.nn.functional.linear(input, weight, bias=None)
torch.nn.functional.local_response_norm(input: torch.Tensor, size: int, alpha: float = 0.0001, beta: float = 0.75, k: float = 1.0)
torch.nn.functional.log_softmax(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None)
torch.nn.functional.logsigmoid(input)
torch.nn.functional.lp_pool1d(input: torch.Tensor, norm_type: Union[int, float], kernel_size: int, stride: NoneType = None, ceil_mode: bool = False)
torch.nn.functional.lp_pool2d(input: torch.Tensor, norm_type: Union[int, float], kernel_size: None, stride: NoneType = None, ceil_mode: bool = False)
torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)
torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)
torch.nn.functional.max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)
torch.nn.functional.max_unpool1d(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None)
torch.nn.functional.max_unpool2d(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None)
torch.nn.functional.max_unpool3d(input: torch.Tensor, indices: torch.Tensor, kernel_size: None, stride: NoneType = None, padding: None = 0, output_size: NoneType = None)
torch.nn.functional.mish(input: torch.Tensor, inplace: bool = False)
torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.nll_loss(input: torch.Tensor, target: torch.Tensor, weight: Optional[torch.Tensor] = None, size_average: Optional[bool] = None, ignore_index: int = -100, reduce: Optional[bool] = None, reduction: str = 'mean')
torch.nn.functional.normalize(input: torch.Tensor, p: float = 2.0, dim: int = 1, eps: float = 1e-12, out: Optional[torch.Tensor] = None)
torch.nn.functional.one_hot(tensor, num_classes=-1)
torch.nn.functional.pad(input, pad, mode="constant", value=None)
torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False)
torch.nn.functional.pdist(input, p=2)
torch.nn.functional.pixel_unshuffle(input, downscale_factor)
torch.nn.functional.poisson_nll_loss(input: torch.Tensor, target: torch.Tensor, log_input: bool = True, full: bool = False, size_average: Optional[bool] = None, eps: float = 1e-08, reduce: Optional[bool] = None, reduction: str = 'mean')
torch.nn.functional.prelu(input, weight)
torch.nn.functional.relu(input, inplace=False)
torch.nn.functional.relu6(input, inplace=False)
torch.nn.functional.relu_(input)
torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False)
torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False)
torch.nn.functional.selu(input, inplace=False)
torch.nn.functional.sigmoid(input)
torch.nn.functional.silu(input: torch.Tensor, inplace: bool = False)
torch.nn.functional.smooth_l1_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean', beta: float = 1.0)
torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')
torch.nn.functional.softmax(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None)
torch.nn.functional.softmin(input: torch.Tensor, dim: Optional[int] = None, _stacklevel: int = 3, dtype: Optional[int] = None)
torch.nn.functional.softplus(input, beta=1, threshold=20)
torch.nn.functional.softshrink(input, lambd=0.5)
torch.nn.functional.softsign(input)
torch.nn.functional.tanh(input)
torch.nn.functional.tanhshrink(input)
torch.nn.functional.threshold(input: torch.Tensor, threshold: float, value: float, inplace: bool = False)
torch.nn.functional.threshold_(input, threshold, value)
torch.nn.functional.triplet_margin_loss(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, margin: float = 1.0, p: float = 2, eps: float = 1e-06, swap: bool = False, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean')
torch.nn.functional.triplet_margin_with_distance_loss(anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor, *, distance_function: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean')
torch.nn.functional.unfold(input: torch.Tensor, kernel_size: None, dilation: None = 1, padding: None = 0, stride: None = 1)
torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)
torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)
torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)
torch.nn.init.calculate_gain(nonlinearity, param=None)
torch.nn.init.constant_(tensor: torch.Tensor, val: float)
torch.nn.init.dirac_(tensor, groups=1)
torch.nn.init.eye_(tensor)
torch.nn.init.kaiming_normal_(tensor: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu', generator: Optional[torch._C.Generator] = None)
torch.nn.init.kaiming_uniform_(tensor: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu', generator: Optional[torch._C.Generator] = None)
torch.nn.init.normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, generator: Optional[torch._C.Generator] = None)
torch.nn.init.ones_(tensor: torch.Tensor)
torch.nn.init.orthogonal_(tensor, gain=1, generator: Optional[torch._C.Generator] = None)
torch.nn.init.sparse_(tensor, sparsity, std=0.01, generator: Optional[torch._C.Generator] = None)
torch.nn.init.uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0, generator: Optional[torch._C.Generator] = None)
torch.nn.init.xavier_normal_(tensor: torch.Tensor, gain: float = 1.0, generator: Optional[torch._C.Generator] = None)
torch.nn.init.xavier_uniform_(tensor: torch.Tensor, gain: float = 1.0, generator: Optional[torch._C.Generator] = None)
torch.nn.init.zeros_(tensor: torch.Tensor)
torch.nn.modules.module.register_module_backward_hook(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])
torch.nn.parameter.UninitializedBuffer(requires_grad=False, device=None, dtype=None)
torch.nn.parameter.UninitializedParameter(requires_grad=True, device=None, dtype=None)
torch.nn.utils.clip_grad_norm_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], max_norm: float, norm_type: float = 2.0, error_if_nonfinite: bool = False, foreach: Optional[bool] = None)
torch.nn.utils.clip_grad_value_(parameters: Union[torch.Tensor, Iterable[torch.Tensor]], clip_value: float, foreach: Optional[bool] = None)
torch.nn.utils.parameters_to_vector(parameters: Iterable[torch.Tensor])
torch.nn.utils.parametrizations.orthogonal(module: torch.nn.modules.module.Module, name: str = 'weight', orthogonal_map: Optional[str] = None, *, use_trivialization: bool = True)
torch.nn.utils.parametrize.cached()
torch.nn.utils.parametrize.is_parametrized(module: torch.nn.modules.module.Module, tensor_name: Optional[str] = None)
torch.nn.utils.remove_weight_norm(module: ~T_module, name: str = 'weight')
torch.nn.utils.rnn.pack_padded_sequence(input: torch.Tensor, lengths: torch.Tensor, batch_first: bool = False, enforce_sorted: bool = True)
torch.nn.utils.rnn.pack_sequence(sequences: List[torch.Tensor], enforce_sorted: bool = True)
torch.nn.utils.rnn.pad_packed_sequence(sequence: torch.nn.utils.rnn.PackedSequence, batch_first: bool = False, padding_value: float = 0.0, total_length: Optional[int] = None)
torch.nn.utils.rnn.pad_sequence(sequences: Union[torch.Tensor, List[torch.Tensor]], batch_first: bool = False, padding_value: float = 0.0)
torch.nn.utils.skip_init(module_cls, *args, **kwargs)
torch.nn.utils.spectral_norm(module: ~T_module, name: str = 'weight', n_power_iterations: int = 1, eps: float = 1e-12, dim: Optional[int] = None)
torch.nn.utils.vector_to_parameters(vec: torch.Tensor, parameters: Iterable[torch.Tensor])
torch.nn.utils.weight_norm(module: ~T_module, name: str = 'weight', dim: int = 0)
torch.no_grad()
torch.nonzero(input, *, out=None, as_tuple=False)
torch.norm(input, p: Union[float, str, NoneType] = 'fro', dim=None, keepdim=False, out=None, dtype=None)
torch.normal(mean, std, *, generator=None, out=None)
torch.not_equal(input, other, *, out=None)
torch.numel(input)
torch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)
torch.onnx.is_in_onnx_export()
torch.optim.ASGD(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.01, lambd: float = 0.0001, alpha: float = 0.75, t0: float = 1000000.0, weight_decay: float = 0, foreach: Optional[bool] = None, maximize: bool = False, differentiable: bool = False, capturable: bool = False)
torch.optim.Adadelta(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 1.0, rho: float = 0.9, eps: float = 1e-06, weight_decay: float = 0, foreach: Optional[bool] = None, *, capturable: bool = False, maximize: bool = False, differentiable: bool = False)
torch.optim.Adagrad(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.01, lr_decay: float = 0, weight_decay: float = 0, initial_accumulator_value: float = 0, eps: float = 1e-10, foreach: Optional[bool] = None, *, maximize: bool = False, differentiable: bool = False, fused: Optional[bool] = None)
torch.optim.Adam(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: Union[float, torch.Tensor] = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, amsgrad: bool = False, *, foreach: Optional[bool] = None, maximize: bool = False, capturable: bool = False, differentiable: bool = False, fused: Optional[bool] = None)
torch.optim.AdamW(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: Union[float, torch.Tensor] = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0.01, amsgrad: bool = False, *, maximize: bool = False, foreach: Optional[bool] = None, capturable: bool = False, differentiable: bool = False, fused: Optional[bool] = None)
torch.optim.Adamax(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.002, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, foreach: Optional[bool] = None, *, maximize: bool = False, differentiable: bool = False, capturable: bool = False)
torch.optim.NAdam(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.002, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, momentum_decay: float = 0.004, decoupled_weight_decay: bool = False, *, foreach: Optional[bool] = None, maximize: bool = False, capturable: bool = False, differentiable: bool = False)
torch.optim.RAdam(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, weight_decay: float = 0, decoupled_weight_decay: bool = False, *, foreach: Optional[bool] = None, maximize: bool = False, capturable: bool = False, differentiable: bool = False)
torch.optim.RMSprop(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.01, alpha: float = 0.99, eps: float = 1e-08, weight_decay: float = 0, momentum: float = 0, centered=False, capturable=False, foreach: Optional[bool] = None, maximize: bool = False, differentiable: bool = False)
torch.optim.Rprop(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.01, etas: Tuple[float, float] = (0.5, 1.2), step_sizes: Tuple[float, float] = (1e-06, 50), *, capturable: bool = False, foreach: Optional[bool] = None, maximize: bool = False, differentiable: bool = False)
torch.optim.SGD(params, lr: float = 0.001, momentum: float = 0, dampening: float = 0, weight_decay: float = 0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None, differentiable: bool = False, fused: Optional[bool] = None)
torch.optim.SparseAdam(params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]], lr: float = 0.001, betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-08, maximize: bool = False)
torch.optim.lr_scheduler.ChainedScheduler(schedulers: Sequence[torch.optim.lr_scheduler.LRScheduler], optimizer: Optional[torch.optim.optimizer.Optimizer] = None)
torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer: torch.optim.optimizer.Optimizer, mode: Literal['min', 'max'] = 'min', factor=0.1, patience=10, threshold=0.0001, threshold_mode: Literal['rel', 'abs'] = 'rel', cooldown=0, min_lr: Union[List[float], float] = 0, eps=1e-08, verbose='deprecated')
torch.orgqr(input, tau)
torch.ormqr(input, tau, other, left=True, transpose=False, *, out=None)
torch.outer(input, vec2, *, out=None)
torch.overrides.get_ignored_functions()
torch.overrides.handle_torch_function(public_api: Callable, relevant_args: Iterable[Any], *args, **kwargs)
torch.overrides.has_torch_function()
torch.overrides.is_tensor_like(inp)
torch.overrides.wrap_torch_function(dispatcher: Callable)
torch.package.Directory(name: str, is_dir: bool)
torch.package.EmptyMatchError(builtins.Exception)
torch.package.PackagingError(dependency_graph: torch.package._digraph.DiGraph, debug=False)
torch.pca_lowrank(A: torch.Tensor, q: Optional[int] = None, center: bool = True, niter: int = 2)
torch.permute(input, dims)
torch.pinverse(input, rcond=1e-15)
torch.poisson(input, generator=None)
torch.polar(abs, angle, *, out=None)
torch.polygamma(n, input, *, out=None)
torch.positive(input)
torch.pow(input, exponent, *, out=None)
torch.prod(input, *, dtype=None)
torch.profiler.profile(*, activities: Optional[Iterable[torch._C._profiler.ProfilerActivity]] = None, schedule: Optional[Callable[[int], torch.profiler.profiler.ProfilerAction]] = None, on_trace_ready: Optional[Callable[..., Any]] = None, record_shapes: bool = False, profile_memory: bool = False, with_stack: bool = False, with_flops: bool = False, with_modules: bool = False, experimental_config: Optional[torch._C._profiler._ExperimentalConfig] = None, execution_trace_observer: Optional[torch.profiler.profiler._ITraceObserver] = None, use_cuda: Optional[bool] = None)
torch.profiler.tensorboard_trace_handler(dir_name: str, worker_name: Optional[str] = None, use_gzip: bool = False)
torch.promote_types(type1, type2)
torch.qr(input, some=True, *, out=None)
torch.quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None)
torch.quantize_per_channel(input, scales, zero_points, axis, dtype)
torch.quantize_per_tensor(input, scale, zero_point, dtype)
torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)
torch.rad2deg(input, *, out=None)
torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)
torch.rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)
torch.randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.randint_like(input, low=0, high, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format)
torch.randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)
torch.randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)
torch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices', device_type='cuda')
torch.random.get_rng_state()
torch.random.initial_seed()
torch.random.manual_seed(seed)
torch.random.seed()
torch.random.set_rng_state(new_state: torch.Tensor)
torch.randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=None, requires_grad=False, pin_memory=False)
torch.range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.ravel(input)
torch.real(input)
torch.reciprocal(input, *, out=None)
torch.remainder(input, other, *, out=None)
torch.renorm(input, p, dim, maxnorm, *, out=None)
torch.repeat_interleave(input, repeats, dim=None, *, output_size=None)
torch.reshape(input, shape)
torch.resolve_conj(input)
torch.resolve_neg(input)
torch.result_type(tensor1, tensor2)
torch.roll(input, shifts, dims=None)
torch.rot90(input, k=1, dims=[0,1])
torch.round(input, *, decimals=0, out=None)
torch.row_stack(tensors, *, out=None)
torch.rsqrt(input, *, out=None)
torch.scatter(input, dim, index, src)
torch.scatter_add(input, dim, index, src)
torch.searchsorted(sorted_sequence, values, *, out_int32=False, right=False, side=None, out=None, sorter=None)
torch.seed()
torch.set_default_dtype(d)
torch.set_default_tensor_type(t)
torch.set_flush_denormal(mode)
torch.set_grad_enabled(mode: bool)
torch.set_num_interop_threads(int)
torch.set_num_threads(int)
torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)
torch.set_rng_state(new_state: torch.Tensor)
torch.set_warn_always(b: bool)
torch.sgn(input, *, out=None)
torch.sigmoid(input, *, out=None)
torch.sign(input, *, out=None)
torch.signbit(input, *, out=None)
torch.sin(input, *, out=None)
torch.sinh(input, *, out=None)
torch.slogdet(input)
torch.sort(input, dim=-1, descending=False, stable=False, *, out=None)
torch.sparse.addmm(mat, mat1, mat2, *, beta=1., alpha=1.)
torch.sparse.log_softmax(input, dim, *, dtype=None)
torch.sparse.mm()
torch.sparse.sum(input: torch.Tensor, dim: Optional[Tuple[int]] = None, dtype: Optional[int] = None)
torch.sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False, check_invariants=None, is_coalesced=None)
torch.special.digamma(input, *, out=None)
torch.special.entr(input, *, out=None)
torch.special.erf(input, *, out=None)
torch.special.erfc(input, *, out=None)
torch.special.erfcx(input, *, out=None)
torch.special.erfinv(input, *, out=None)
torch.special.exp2(input, *, out=None)
torch.special.expit(input, *, out=None)
torch.special.expm1(input, *, out=None)
torch.special.gammainc(input, other, *, out=None)
torch.special.gammaincc(input, other, *, out=None)
torch.special.gammaln(input, *, out=None)
torch.special.i0(input, *, out=None)
torch.special.i0e(input, *, out=None)
torch.special.i1(input, *, out=None)
torch.special.i1e(input, *, out=None)
torch.special.log1p(input, *, out=None)
torch.special.log_softmax(input, dim, *, dtype=None)
torch.special.logit(input, eps=None, *, out=None)
torch.special.logsumexp(input, dim, keepdim=False, *, out=None)
torch.special.ndtr(input, *, out=None)
torch.special.ndtri(input, *, out=None)
torch.special.polygamma(n, input, *, out=None)
torch.special.psi(input, *, out=None)
torch.special.round(input, *, out=None)
torch.special.sinc(input, *, out=None)
torch.special.xlog1py(input, other, *, out=None)
torch.special.xlogy(input, other, *, out=None)
torch.special.zeta(input, other, *, out=None)
torch.split(tensor: torch.Tensor, split_size_or_sections: Union[int, List[int]], dim: int = 0)
torch.sqrt(input, *, out=None)
torch.square(input, *, out=None)
torch.squeeze(input, dim=None)
torch.stack(tensors, dim=0, *, out=None)
torch.std(input, dim=None, *, correction=1, keepdim=False, out=None)
torch.std_mean(input, dim=None, *, correction=1, keepdim=False, out=None)
torch.stft(input: torch.Tensor, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: Optional[torch.Tensor] = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)
torch.sub(input, other, *, alpha=1, out=None)
torch.subtract(input, other, *, alpha=1, out=None)
torch.sum(input, *, dtype=None)
torch.svd(input, some=True, compute_uv=True, *, out=None)
torch.svd_lowrank(A: torch.Tensor, q: Optional[int] = 6, niter: Optional[int] = 2, M: Optional[torch.Tensor] = None)
torch.swapaxes(input, axis0, axis1)
torch.swapdims(input, dim0, dim1)
torch.symeig(input, eigenvectors=False, upper=True, *, out=None)
torch.t(input)
torch.take(input, index)
torch.tan(input, *, out=None)
torch.tanh(input, *, out=None)
torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False)
torch.tensor_split(input, indices_or_sections, dim=0)
torch.tensordot(a, b, dims=2, out: Optional[torch.Tensor] = None)
torch.testing.assert_close(actual: Any, expected: Any, *, allow_subclasses: bool = True, rtol: Optional[float] = None, atol: Optional[float] = None, equal_nan: bool = False, check_device: bool = True, check_dtype: bool = True, check_layout: bool = True, check_stride: bool = False, msg: Union[str, Callable[[str], str], NoneType] = None)
torch.testing.make_tensor(*shape: Union[int, torch.Size, List[int], Tuple[int, ...]], dtype: torch.dtype, device: Union[str, torch.device], low: Optional[float] = None, high: Optional[float] = None, requires_grad: bool = False, noncontiguous: bool = False, exclude_zero: bool = False, memory_format: Optional[torch.memory_format] = None)
torch.tile(input, dims)
torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None)
torch.trace(input)
torch.transpose(input, dim0, dim1)
torch.trapezoid(y, x=None, *, dx=None, dim=-1)
torch.trapz(y, x, *, dim=-1)
torch.triangular_solve(b, A, upper=True, transpose=False, unitriangular=False, *, out=None)
torch.tril(input, diagonal=0, *, out=None)
torch.tril_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided)
torch.triu(input, diagonal=0, *, out=None)
torch.triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided)
torch.true_divide(dividend, divisor, *, out)
torch.trunc(input, *, out=None)
torch.unbind(input, dim=0)
torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None)
torch.unique_consecutive(*args, **kwargs)
torch.unsqueeze(input, dim)
torch.use_deterministic_algorithms(mode: bool, *, warn_only: bool = False)
torch.utils.data.BatchSampler(sampler: Union[torch.utils.data.sampler.Sampler[int], Iterable[int]], batch_size: int, drop_last: bool)
torch.utils.data.ChainDataset(datasets: Iterable[torch.utils.data.dataset.Dataset])
torch.utils.data.ConcatDataset(datasets: Iterable[torch.utils.data.dataset.Dataset])
torch.utils.data.DataLoader(dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '')
torch.utils.data.RandomSampler(data_source: Sized, replacement: bool = False, num_samples: Optional[int] = None, generator=None)
torch.utils.data.Sampler(data_source: Optional[Sized] = None)
torch.utils.data.SequentialSampler(data_source: Sized)
torch.utils.data.Subset(dataset: torch.utils.data.dataset.Dataset[+T_co], indices: Sequence[int])
torch.utils.data.SubsetRandomSampler(indices: Sequence[int], generator=None)
torch.utils.data.TensorDataset(*tensors: torch.Tensor)
torch.utils.data.WeightedRandomSampler(weights: Sequence[float], num_samples: int, replacement: bool = True, generator=None)
torch.utils.data.distributed.DistributedSampler(dataset: torch.utils.data.dataset.Dataset, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False)
torch.utils.data.get_worker_info()
torch.utils.data.random_split(dataset: torch.utils.data.dataset.Dataset[~T], lengths: Sequence[Union[int, float]], generator: Optional[torch._C.Generator] = <torch._C.Generator object at 0x10937d7f0>)
torch.utils.dlpack.from_dlpack(ext_tensor)
torch.utils.dlpack.to_dlpack(tensor)
torch.utils.model_zoo.load_url()
torch.vander(x, N=None, increasing=False)
torch.var(input, dim=None, *, correction=1, keepdim=False, out=None)
torch.var_mean(input, dim=None, *, correction=1, keepdim=False, out=None)
torch.vdot(input, other, *, out=None)
torch.view_as_complex(input)
torch.vsplit(input, indices_or_sections)
torch.vstack(tensors, *, out=None)
torch.where(condition, input, other, *, out=None)
torch.xlogy(input, other, *, out=None)
torch.zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)
torch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)
